{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the VGG model from the module vgg.py\n",
    "from vgg import *\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN, TensorBoard,ReduceLROnPlateau\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os, sys\n",
    "import copy\n",
    "\n",
    "from loguru import logger\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "logger.debug('All modules imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current working directory from which main.py is located\n",
    "cur_dir = os.getcwd()\n",
    "\n",
    "# the data is located in this data_dir\n",
    "data_dir = os.path.join(cur_dir, 'Dataset')\n",
    "\n",
    "# the output model and the graph is saved in this 'output_dir'\n",
    "output_dir = os.path.join(cur_dir, 'Output')\n",
    "\n",
    "logger.debug(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# grab the image paths and shuffle them\n",
    "imagePaths = sorted(list(paths.list_images(data_dir) ))\n",
    "random.seed(2)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 64, 64\n",
    "\n",
    "# loop over the input image paths\n",
    "for imagePath in imagePaths:\n",
    "\n",
    "    # load the image\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    # resize it\n",
    "    image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    \n",
    "    # append to the data list\n",
    "    data.append(image)\n",
    "\n",
    "    # extract the class label from the image path and update the labels list\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "\n",
    "logger.debug('[INFO] data loaded complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row,col = 2,6\n",
    "fig, axs = plt.subplots(row, col, figsize=(15, 7))\n",
    "    \n",
    "count = 0\n",
    "for r in range(row):\n",
    "    for ax in axs[r] :\n",
    "        ax.imshow(cv2.cvtColor(data[count], cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(labels[count])\n",
    "        ax.grid(False)\n",
    "        count = count + 1        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Binarize labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# save the encoder to output directory\n",
    "with open(os.path.join(output_dir,'labels'), 'wb') as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Randomly split the data into test and train sets (15% test and 85% train)\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=45, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "    horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = VGGNet.build(width=IMAGE_WIDTH, height=IMAGE_HEIGHT, depth=3, classes=len(lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our initial learning rate, # of epochs to train for,and batch size\n",
    "INIT_LR = 0.0007\n",
    "EPOCHS = 100\n",
    "BS = 64\n",
    "\n",
    "# Checkpoints between the training steps\n",
    "model_checkpoint = ModelCheckpoint(filepath='VGG_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=20)\n",
    "# Termination of training if the loss become Nan\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "# For watching the live loss, accuracy and graphs using tensorboard\n",
    "t_board = TensorBoard(log_dir='./logs', \n",
    "                      histogram_freq=0,\n",
    "                      batch_size=32, \n",
    "                      write_graph=True, \n",
    "                      write_grads=False,\n",
    "                      write_images=False, \n",
    "                      embeddings_freq=0, \n",
    "                      update_freq='epoch')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=0.00001)\n",
    "\n",
    "callbacks = [model_checkpoint, t_board, terminate_on_nan, reduce_lr]\n",
    "\n",
    "# initialize the model and optimizers\n",
    "opt = Adam(lr=INIT_LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "logger.debug('Training the network...')\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS,callbacks=callbacks)\n",
    "\n",
    "\n",
    "# Save the model locally for use later\n",
    "model_path = os.path.join(output_dir,  'trained_VGG_model.h5')\n",
    "model.save(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "logger.debug('Making predictions and evaluating the trained model.')\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1), target_names=lb.classes_))\n",
    "\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training/Validation Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir ,'vggnet_plot.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
